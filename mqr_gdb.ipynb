{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from neo4j import GraphDatabase\n",
    "from typing import Any, Dict\n",
    "from pydantic import BaseModel, validator, Field\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain_core.documents import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess data from various sources\n",
    "def load_data():\n",
    "    # Load CSV files\n",
    "    united_dates_locations = pd.read_csv(\"data/united_dates_locations.csv\")\n",
    "    alliance_dates_locations = pd.read_csv(\"data/alliance_dates_locations.csv\")\n",
    "    air_canada_dates_locations = pd.read_csv(\"data/air_canada_dates_locations.csv\")\n",
    "\n",
    "    # Load text files\n",
    "    with open(\"data/united_aircraft_details.txt\", \"r\") as file:\n",
    "        united_aircraft_details_content = file.read().split('\\n\\n')\n",
    "    with open(\"data/alliance_aircraft_details.txt\", \"r\") as file:\n",
    "        alliance_aircraft_details_content = file.read().split('\\n\\n')\n",
    "    with open(\"data/air_canada_aircraft_details.txt\", \"r\") as file:\n",
    "        air_canada_aircraft_details_content = file.read().split('\\n\\n')\n",
    "\n",
    "    # Extract text from PDF files\n",
    "    def extract_text_from_pdf(pdf_path):\n",
    "        text = []\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PdfReader(file)\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                text.append(reader.pages[page_num].extract_text())\n",
    "        return \"\\n\".join(text)\n",
    "\n",
    "    united_pdf_content = extract_text_from_pdf(\"data/united_accident_outcomes.pdf\")\n",
    "    alliance_pdf_content = extract_text_from_pdf(\"data/alliance_accident_outcomes.pdf\")\n",
    "    air_canada_pdf_content = extract_text_from_pdf(\"data/air_canada_accident_outcomes.pdf\")\n",
    "\n",
    "    # Combine data\n",
    "    united_data = united_dates_locations['summary'].tolist() + united_aircraft_details_content + [united_pdf_content]\n",
    "    alliance_data = alliance_dates_locations['summary'].tolist() + alliance_aircraft_details_content + [alliance_pdf_content]\n",
    "    air_canada_data = air_canada_dates_locations['summary'].tolist() + air_canada_aircraft_details_content + [air_canada_pdf_content]\n",
    "\n",
    "    return united_data, alliance_data, air_canada_data\n",
    "\n",
    "# Load the data\n",
    "united_data, alliance_data, air_canada_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load models\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "rel_extractor = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Load Hugging Face local embeddings\n",
    "hugging_face_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize the generative model\n",
    "generative_model = Ollama(model=\"gemma:7b\")\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an AI assistant that specializes in providing detailed information about airline accidents. \n",
    "    When given a query about a specific flight, you should:\n",
    "\n",
    "    1. Identify the flight number and any other relevant details from the query.\n",
    "    2. Retrieve specific information about the flight from the provided context, including any relevant accidents or incidents.\n",
    "    3. Summarize the information in a clear and concise manner.\n",
    "    4. If there are multiple incidents related to the flight, provide details on each incident separately.\n",
    "    5. Ensure the response is focused on the specific flight mentioned in the query.\n",
    "\n",
    "    Use the provided context to generate the response and avoid including unrelated information.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "# Initialize document stores\n",
    "class InMemoryDocstore:\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "\n",
    "    def get_document(self, doc_id):\n",
    "        return self.documents.get(doc_id, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n",
      "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n",
      "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "def vectorize_and_index(data_list, index_path):\n",
    "    embeddings = np.vstack([embed_text(text) for text in data_list])\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(embeddings.astype('float32'))\n",
    "    faiss.write_index(index, index_path)\n",
    "\n",
    "vectorize_and_index(united_data, \"faiss_indexes/united_faiss.index\")\n",
    "vectorize_and_index(alliance_data, \"faiss_indexes/alliance_faiss.index\")\n",
    "vectorize_and_index(air_canada_data, \"faiss_indexes/air_canada_faiss.index\")\n",
    "\n",
    "# Initialize document stores\n",
    "united_docstore = InMemoryDocstore({str(i): Document(page_content=text) for i, text in enumerate(united_data)})\n",
    "alliance_docstore = InMemoryDocstore({str(i): Document(page_content=text) for i, text in enumerate(alliance_data)})\n",
    "air_canada_docstore = InMemoryDocstore({str(i): Document(page_content=text) for i, text in enumerate(air_canada_data)})\n",
    "\n",
    "def create_faiss_retriever(index_path, docstore, data_list):\n",
    "    index = faiss.read_index(index_path)\n",
    "    index_to_docstore_id = {i: str(i) for i in range(len(data_list))}\n",
    "    return FAISS(embedding_function=embed_text, index=index, docstore=docstore, index_to_docstore_id=index_to_docstore_id)\n",
    "\n",
    "united_faiss_retriever = create_faiss_retriever(\"faiss_indexes/united_faiss.index\", united_docstore, united_data)\n",
    "alliance_faiss_retriever = create_faiss_retriever(\"faiss_indexes/alliance_faiss.index\", alliance_docstore, alliance_data)\n",
    "air_canada_faiss_retriever = create_faiss_retriever(\"faiss_indexes/air_canada_faiss.index\", air_canada_docstore, air_canada_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDBClient:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def query(self, query, parameters=None):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query, parameters)\n",
    "            return [record for record in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_knowledge_graph(client, keyword):\n",
    "    query = \"\"\"\n",
    "    MATCH (e:Entity)-[r]->(related:Entity)\n",
    "    WHERE e.name =~ $name OR related.name =~ $name\n",
    "    RETURN e.name AS entity, type(r) AS relationship, related.name AS related_entity\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = client.query(query, parameters={\"name\": f\"(?i).*{keyword}.*\"})\n",
    "        return [record for record in result]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying knowledge graph: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_120478/726269644.py:5: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  @validator('faiss_retrievers')\n"
     ]
    }
   ],
   "source": [
    "class CustomMultiRetriever(BaseModel):\n",
    "    faiss_retrievers: Dict[str, FAISS] = Field(..., description=\"Dictionary of FAISS retrievers\")\n",
    "    knowledge_graph_client: Any\n",
    "\n",
    "    @validator('faiss_retrievers')\n",
    "    def check_faiss_retrievers(cls, value):\n",
    "        if not value:\n",
    "            raise ValueError(\"FAISS retrievers cannot be empty\")\n",
    "        return value\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        query_vector = embed_text(query).reshape(1, -1)\n",
    "        results = {}\n",
    "        for airline, retriever in self.faiss_retrievers.items():\n",
    "            distances, indices = retriever.index.search(query_vector, k=10)\n",
    "            docs = [retriever.docstore.get_document(str(i)) for i in indices[0] if retriever.docstore.get_document(str(i)) is not None]\n",
    "            results[airline] = docs\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_graph_and_docs(graph_results, retrieved_docs):\n",
    "    integrated_context = \"Retrieved Context:\\n\"\n",
    "    if graph_results:\n",
    "        for result in graph_results:\n",
    "            entity = result.get('entity', 'Unknown Entity')\n",
    "            relationship = result.get('relationship', 'Unknown Relationship')\n",
    "            related_entity = result.get('related_entity', 'Unknown Related Entity')\n",
    "            integrated_context += f\"{entity} {relationship} {related_entity}. \"\n",
    "    else:\n",
    "        integrated_context += \"No relevant data found in the knowledge graph.\\n\"\n",
    "    \n",
    "    if retrieved_docs:\n",
    "        integrated_context += \"\\nFrom FAISS:\\n\"\n",
    "        for doc in retrieved_docs:\n",
    "            integrated_context += doc.page_content + \"\\n\"\n",
    "    else:\n",
    "        integrated_context += \"No relevant data found in the FAISS retrievers.\\n\"\n",
    "    \n",
    "    return integrated_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(query):\n",
    "    doc = nlp(query)\n",
    "    keywords = [ent.text for ent in doc.ents if ent.label_ in (\"ORG\", \"GPE\", \"NORP\", \"PRODUCT\", \"EVENT\")]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(user_query, custom_multi_retriever, graphdb_client, llm_chain):\n",
    "    try:\n",
    "        entities = extract_entities(user_query)\n",
    "        logger.info(f\"Extracted entities: {entities}\")\n",
    "\n",
    "        graph_context = \"\"\n",
    "        for entity in entities:\n",
    "            graph_results = query_knowledge_graph(graphdb_client, entity)\n",
    "            if (len(graph_results) > 0):\n",
    "                graph_context += integrate_graph_and_docs(graph_results, [])\n",
    "            else:\n",
    "                graph_context += f\"No relevant data found in the knowledge graph for entity: {entity}.\\n\"\n",
    "\n",
    "        faiss_docs = custom_multi_retriever.get_relevant_documents(user_query)\n",
    "\n",
    "        final_context = graph_context + \" \".join([doc.page_content for docs in faiss_docs.values() for doc in docs])\n",
    "\n",
    "        response = llm_chain({\"context\": final_context})\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing query: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Initialize connection to Neo4j\n",
    "graphdb_client = GraphDBClient(uri=os.getenv('NEO4J_URI'), user=os.getenv('NEO4J_USERNAME'), password=os.getenv('NEO4J_PASSWORD'))\n",
    "\n",
    "custom_multi_retriever = CustomMultiRetriever(\n",
    "    faiss_retrievers={\n",
    "        \"United Airlines\": united_faiss_retriever,\n",
    "        \"Alliance Airlines\": alliance_faiss_retriever,\n",
    "        \"Air Canada\": air_canada_faiss_retriever\n",
    "    },\n",
    "    knowledge_graph_client=graphdb_client\n",
    ")\n",
    "\n",
    "# Initialize connection to Neo4j\n",
    "graphdb_client = GraphDBClient(uri=os.getenv('NEO4J_URI'), user=os.getenv('NEO4J_USERNAME'), password=os.getenv('NEO4J_PASSWORD'))\n",
    "\n",
    "# Create the LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt_template, llm=generative_model)\n",
    "\n",
    "user_query = \"Tell me about what aircraft FLIGHT 624 faced accident\"\n",
    "response = process_query(user_query, custom_multi_retriever, graphdb_client, llm_chain)\n",
    "print(f\"Response: {response['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: **Query:** Provide information about United Airlines Flight 328.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "United Airlines Flight 328 experienced two incidents:\n",
      "\n",
      "**1. Right Engine Failure (Accident ID: 3)**\n",
      "\n",
      "* Shortly after takeoff, the aircraft encountered a right engine failure.\n",
      "* Debris from the engine fell onto residential areas below.\n",
      "* The aircraft returned to Denver International Airport for an emergency landing.\n",
      "* There were no injuries among the passengers or crew.\n",
      "\n",
      "\n",
      "**2. Emergency Landing (Accident ID: 3)**\n",
      "\n",
      "* The flight experienced an unspecified engine issue and declared an emergency landing.\n",
      "* The aircraft returned to Denver International Airport without further incident.\n",
      "* The incident led to inspections and modifications to similar engines to prevent future.\n",
      "\n",
      "**Note:** This response provides a comprehensive summary of the incident.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Tell me about what aircraft FLIGHT 328\"\n",
    "response = process_query(user_query, custom_multi_retriever, graphdb_client, llm_chain)\n",
    "print(f\"Response: {response['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Flight 328 Summary:**\\n\\nUnited Airlines Flight 328 experienced a right engine failure shortly after takeoff from Denver International Airport. Debris from the engine fell onto residential areas below. The aircraft returned to the airport for an emergency landing without injuries among the passengers or crew.\\n\\n**Specific Incidents:**\\n\\n**1. Engine Failure:**\\n- The right engine malfunctioned shortly after takeoff.\\n- Debris fell onto surrounding residential areas.\\n\\n**2. Emergency Landing:**\\n- The flight returned to Denver International Airport for an emergency landing.\\n- No injuries were reported among passengers or crew.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
