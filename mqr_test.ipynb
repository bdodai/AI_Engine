{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from typing import Any, Dict\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain_core.documents import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess data from various sources\n",
    "def load_data():\n",
    "    # Load CSV files\n",
    "    united_dates_locations = pd.read_csv(\"data/united_dates_locations.csv\")\n",
    "    alliance_dates_locations = pd.read_csv(\"data/alliance_dates_locations.csv\")\n",
    "    air_canada_dates_locations = pd.read_csv(\"data/air_canada_dates_locations.csv\")\n",
    "\n",
    "    # Load text files\n",
    "    with open(\"data/united_aircraft_details.txt\", \"r\") as file:\n",
    "        united_aircraft_details_content = file.read().split('\\n\\n')\n",
    "    with open(\"data/alliance_aircraft_details.txt\", \"r\") as file:\n",
    "        alliance_aircraft_details_content = file.read().split('\\n\\n')\n",
    "    with open(\"data/air_canada_aircraft_details.txt\", \"r\") as file:\n",
    "        air_canada_aircraft_details_content = file.read().split('\\n\\n')\n",
    "\n",
    "    # Extract text from PDF files\n",
    "    def extract_text_from_pdf(pdf_path):\n",
    "        text = []\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PdfReader(file)\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                text.append(reader.pages[page_num].extract_text())\n",
    "        return \"\\n\".join(text)\n",
    "\n",
    "    united_pdf_content = extract_text_from_pdf(\"data/united_accident_outcomes.pdf\")\n",
    "    alliance_pdf_content = extract_text_from_pdf(\"data/alliance_accident_outcomes.pdf\")\n",
    "    air_canada_pdf_content = extract_text_from_pdf(\"data/air_canada_accident_outcomes.pdf\")\n",
    "\n",
    "    # Combine data\n",
    "    united_data = united_dates_locations['summary'].tolist() + united_aircraft_details_content + [united_pdf_content]\n",
    "    alliance_data = alliance_dates_locations['summary'].tolist() + alliance_aircraft_details_content + [alliance_pdf_content]\n",
    "    air_canada_data = air_canada_dates_locations['summary'].tolist() + air_canada_aircraft_details_content + [air_canada_pdf_content]\n",
    "\n",
    "    return united_data, alliance_data, air_canada_data\n",
    "\n",
    "# Load the data\n",
    "united_data, alliance_data, air_canada_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load models\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "rel_extractor = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Load Hugging Face local embeddings\n",
    "hugging_face_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "# Initialize document stores\n",
    "class InMemoryDocstore:\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "\n",
    "    def get_document(self, doc_id):\n",
    "        return self.documents.get(doc_id, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n",
      "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n",
      "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "def vectorize_and_index(data_list, index_path):\n",
    "    embeddings = np.vstack([embed_text(text) for text in data_list])\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(embeddings.astype('float32'))\n",
    "    faiss.write_index(index, index_path)\n",
    "\n",
    "vectorize_and_index(united_data, \"faiss_indexes/united_faiss.index\")\n",
    "vectorize_and_index(alliance_data, \"faiss_indexes/alliance_faiss.index\")\n",
    "vectorize_and_index(air_canada_data, \"faiss_indexes/air_canada_faiss.index\")\n",
    "\n",
    "# Initialize document stores\n",
    "united_docstore = InMemoryDocstore({str(i): Document(page_content=text, metadata={\"doc_id\": f\"united_{i}\"}) for i, text in enumerate(united_data)})\n",
    "alliance_docstore = InMemoryDocstore({str(i): Document(page_content=text, metadata={\"doc_id\": f\"alliance_{i}\"}) for i, text in enumerate(alliance_data)})\n",
    "air_canada_docstore = InMemoryDocstore({str(i): Document(page_content=text, metadata={\"doc_id\": f\"air_canada_{i}\"}) for i, text in enumerate(air_canada_data)})\n",
    "\n",
    "def create_faiss_retriever(index_path, docstore, data_list):\n",
    "    index = faiss.read_index(index_path)\n",
    "    index_to_docstore_id = {i: str(i) for i in range(len(data_list))}\n",
    "    return FAISS(embedding_function=embed_text, index=index, docstore=docstore, index_to_docstore_id=index_to_docstore_id)\n",
    "\n",
    "united_faiss_retriever = create_faiss_retriever(\"faiss_indexes/united_faiss.index\", united_docstore, united_data)\n",
    "alliance_faiss_retriever = create_faiss_retriever(\"faiss_indexes/alliance_faiss.index\", alliance_docstore, alliance_data)\n",
    "air_canada_faiss_retriever = create_faiss_retriever(\"faiss_indexes/air_canada_faiss.index\", air_canada_docstore, air_canada_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDBClient:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.uri = uri\n",
    "        self.username = user\n",
    "        self.password = password\n",
    "        self.driver = None\n",
    "        self.connect()\n",
    "\n",
    "    def connect(self):\n",
    "        try:\n",
    "            self.driver = GraphDatabase.driver(self.uri, auth=(self.username, self.password))\n",
    "            logging.info(\"Successfully connected to Neo4j.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to connect to Neo4j: {e}\")\n",
    "            raise ConnectionError(f\"Failed to connect to Neo4j: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "\n",
    "    def query(self, search_terms):\n",
    "        # Define the Cypher query template\n",
    "        query_template = \"\"\"\n",
    "        MATCH (e:Entity)-[r:RELATION]->(related:Entity)\n",
    "        WHERE toLower(e.name) CONTAINS toLower($keyword) \n",
    "           OR toLower(e.type) CONTAINS toLower($keyword)\n",
    "           OR toLower(related.name) CONTAINS toLower($keyword)\n",
    "           OR toLower(related.type) CONTAINS toLower($keyword)\n",
    "           OR toLower(type(r)) CONTAINS toLower($keyword)\n",
    "        RETURN e.name AS entity, e.type AS entity_type, type(r) AS relationship, related.name AS related_entity, related.type AS related_entity_type\n",
    "        \"\"\"\n",
    "        all_records = []\n",
    "        for term in search_terms:\n",
    "            try:\n",
    "                with self.driver.session() as session:\n",
    "                    result = session.run(query_template, keyword=term)\n",
    "                    records = [record.data() for record in result]\n",
    "                    all_records.extend(records)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error querying knowledge graph: {e}\")\n",
    "        return all_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8_/w8p7c86x5_v0716y6rvk1sf40000gn/T/ipykernel_14917/2799301787.py:8: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  @validator('faiss_retrievers')\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "class GraphDBRetriever(BaseModel):\n",
    "    graphdb_client: GraphDBClient\n",
    "    faiss_retrievers: Dict[str, Any] = Field(..., description=\"Dictionary of FAISS retrievers\")\n",
    "\n",
    "    @validator('faiss_retrievers')\n",
    "    def check_faiss_retrievers(cls, value):\n",
    "        if not value:\n",
    "            raise ValueError(\"FAISS retrievers cannot be empty\")\n",
    "        return value\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> Dict[str, List[dict]]:\n",
    "        search_terms = self.extract_search_terms(query)\n",
    "        logging.info(f\"Extracted search terms: {search_terms}\")\n",
    "\n",
    "        # Query the GraphDB\n",
    "        graphdb_results = self.graphdb_client.query(search_terms)\n",
    "\n",
    "        # Query FAISS retrievers\n",
    "        query_vector = embed_text(query).reshape(1, -1)\n",
    "        faiss_results = {}\n",
    "        for name, retriever in self.faiss_retrievers.items():\n",
    "            distances, indices = retriever.index.search(query_vector, k=10)\n",
    "            docs = [retriever.docstore.get_document(str(i)) for i in indices[0] if retriever.docstore.get_document(str(i)) is not None]\n",
    "            faiss_results[name] = docs\n",
    "\n",
    "        return {\n",
    "            \"graphdb\": graphdb_results,\n",
    "            **faiss_results\n",
    "        }\n",
    "\n",
    "    def extract_search_terms(self, query: str) -> List[str]:\n",
    "        prompt_text = \"\"\"\n",
    "        Extract the most relevant and concise search terms from the following user query.\n",
    "        Ensure the search terms are highly relevant, concise, and avoid generating any special characters or logical operators.\n",
    "        Do not provide any other text, just provide the search terms.\n",
    "        User Query: {query}\n",
    "        Search Terms (comma-separated):\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_text)\n",
    "        search_chain = LLMChain(llm=gen_model, prompt=prompt)\n",
    "        search_terms_response = search_chain.run({\"query\": query}).strip()\n",
    "        search_terms = [term.strip() for term in search_terms_response.split(',') if term.strip()]\n",
    "        return search_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Ollama gemma:2b model\n",
    "gen_model = Ollama(model=\"gemma:7b\")\n",
    "\n",
    "# Define a detailed prompt template for extracting entities\n",
    "entity_prompt = PromptTemplate(template=\"\"\"\n",
    "Extract all relevant entities from the provided text and classify them into categories.\n",
    "Text: {text}\n",
    "Entities:\n",
    "\"\"\")\n",
    "\n",
    "relationship_prompt = PromptTemplate(template=\"\"\"\n",
    "Identify the relationship between the following entities based on the provided text.\n",
    "Text: {text}\n",
    "Entity 1: {entity1}\n",
    "Entity 2: {entity2}\n",
    "Relationship:\n",
    "\"\"\")\n",
    "\n",
    "# Define LLMChain for entities and relationships\n",
    "entity_chain = LLMChain(llm=gen_model, prompt=entity_prompt)\n",
    "relationship_chain = LLMChain(llm=gen_model, prompt=relationship_prompt)\n",
    "\n",
    "def extract_entities_with_llm(text):\n",
    "    response = entity_chain.run({\"text\": text})\n",
    "    logging.info(f\"Entity extraction response: {response}\")\n",
    "    entities = []\n",
    "    for line in response.split(\"\\n\"):\n",
    "        if line.strip():\n",
    "            parts = line.split(\":\")\n",
    "            if len(parts) == 2:\n",
    "                entity_text, entity_label = parts[0].strip(), parts[1].strip()\n",
    "                entities.append((entity_text, entity_label))\n",
    "    return entities\n",
    "\n",
    "def extract_relationships_with_llm(text, entities):\n",
    "    relationships = []\n",
    "    for i in range(len(entities) - 1):\n",
    "        for j in range(i + 1, len(entities)):\n",
    "            entity1, entity2 = entities[i][0], entities[j][0]\n",
    "            response = relationship_chain.run({\"text\": text, \"entity1\": entity1, \"entity2\": entity2})\n",
    "            logging.info(f\"Relationship extraction response for {entity1} and {entity2}: {response}\")\n",
    "            relationship = response.strip()\n",
    "            relationships.append((entity1, relationship, entity2))\n",
    "    return relationships\n",
    "\n",
    "def extract_entities_and_relationships(text):\n",
    "    entities = extract_entities_with_llm(text)\n",
    "    relationships = extract_relationships_with_llm(text, entities)\n",
    "    return entities, relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_knowledge_graph(conn, datasets):\n",
    "    try:\n",
    "        with conn.driver.session() as session:\n",
    "            for dataset_name, data in datasets.items():\n",
    "                for entry in data:\n",
    "                    text_data = entry  # Using the text entry as the input text\n",
    "                    \n",
    "                    # Extract entities and relationships\n",
    "                    entities, relationships = extract_entities_and_relationships(text_data)\n",
    "                    \n",
    "                    # Log extracted entities and relationships\n",
    "                    logging.info(f\"Extracted entities: {entities}\")\n",
    "                    logging.info(f\"Extracted relationships: {relationships}\")\n",
    "                    \n",
    "                    # Create Dataset node\n",
    "                    session.run(\"\"\"\n",
    "                    MERGE (d:Dataset {name: $dataset_name})\n",
    "                    RETURN d\n",
    "                    \"\"\", parameters={'dataset_name': dataset_name})\n",
    "                    logging.info(f\"Dataset node created for {dataset_name}\")\n",
    "                    \n",
    "                    # Create Entry node\n",
    "                    session.run(\"\"\"\n",
    "                    MERGE (e:Entry {text: $text_data})\n",
    "                    RETURN e\n",
    "                    \"\"\", parameters={'text_data': text_data})\n",
    "                    logging.info(f\"Entry node created for {text_data}\")\n",
    "                    \n",
    "                    # Create relationships between Dataset and Entry\n",
    "                    session.run(\"\"\"\n",
    "                    MATCH (d:Dataset {name: $dataset_name})\n",
    "                    MATCH (e:Entry {text: $text_data})\n",
    "                    MERGE (d)-[:CONTAINS_ENTRY]->(e)\n",
    "                    \"\"\", parameters={'dataset_name': dataset_name, 'text_data': text_data})\n",
    "                    logging.info(f\"Relationship created between Dataset {dataset_name} and Entry {text_data}\")\n",
    "                    \n",
    "                    # Create entities and relationships to the Entry\n",
    "                    for entity_text, entity_label in entities:\n",
    "                        session.run(\"\"\"\n",
    "                        MERGE (en:Entity {name: $entity, type: $type})\n",
    "                        MERGE (e:Entry {text: $text_data})-[:MENTIONS]->(en)\n",
    "                        \"\"\", parameters={'entity': entity_text, 'type': entity_label, 'text_data': text_data})\n",
    "                        logging.info(f\"Entity node created for {entity_text} with label {entity_label}, related to Entry {text_data}\")\n",
    "                    \n",
    "                    # Create relationships between entities based on extracted relationships\n",
    "                    for entity1, relation, entity2 in relationships:\n",
    "                        session.run(\"\"\"\n",
    "                        MATCH (en1:Entity {name: $entity1}), (en2:Entity {name: $entity2})\n",
    "                        MERGE (en1)-[r:RELATION {type: $relation, source: $source}]->(en2)\n",
    "                        \"\"\", parameters={'entity1': entity1, 'entity2': entity2, 'relation': relation, 'source': dataset_name})\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating knowledge graph: {e}\")\n",
    "\n",
    "# Initialize connection to Neo4j\n",
    "graphdb_client = GraphDBClient(uri=os.getenv('NEO4J_URI'), user=os.getenv('NEO4J_USERNAME'), password=os.getenv('NEO4J_PASSWORD'))\n",
    "\n",
    "# Combine all datasets\n",
    "datasets = {\n",
    "    \"United_Airlines\": united_data,\n",
    "    \"Alliance_Airlines\": alliance_data,\n",
    "    \"Air_Canada\": air_canada_data\n",
    "}\n",
    "#create_knowledge_graph(graphdb_client, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_graph_and_docs(graph_results: List[dict], retrieved_docs: Dict[str, List[dict]]) -> str:\n",
    "    context_parts = []\n",
    "\n",
    "    if graph_results:\n",
    "        context_parts.append(\"From GraphDB:\\n\")\n",
    "        for result in graph_results:\n",
    "            entity = result.get('entity', 'Unknown Entity')\n",
    "            relationship = result.get('relationship', 'Unknown Relationship')\n",
    "            related_entity = result.get('related_entity', 'Unknown Related Entity')\n",
    "            context_parts.append(f\"{entity} {relationship} {related_entity}.\\n\")\n",
    "    else:\n",
    "        context_parts.append(\"No relevant data found in the knowledge graph.\\n\")\n",
    "\n",
    "    if retrieved_docs:\n",
    "        context_parts.append(\"\\nFrom FAISS:\\n\")\n",
    "        for name, docs in retrieved_docs.items():\n",
    "            context_parts.append(f\"\\n{name}:\\n\")\n",
    "            for doc in docs:\n",
    "                context_parts.append(f\"{doc.page_content}\\n\")\n",
    "    else:\n",
    "        context_parts.append(\"No relevant data found in the FAISS retrievers.\\n\")\n",
    "\n",
    "    return ''.join(context_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(user_query: str, graphdb_retriever: GraphDBRetriever, llm_chain: LLMChain) -> (str, Dict[str, List[str]]):\n",
    "    try:\n",
    "        results = graphdb_retriever.get_relevant_documents(user_query)\n",
    "        graph_results = results.get('graphdb', [])\n",
    "        faiss_results = {key: docs for key, docs in results.items() if key != 'graphdb'}\n",
    "\n",
    "        final_context = integrate_graph_and_docs(graph_results, faiss_results)\n",
    "        response = llm_chain({\"context\": final_context, \"user_query\": user_query})\n",
    "\n",
    "        retrieved_doc_ids = {key: [doc.metadata[\"doc_id\"] for doc in docs] for key, docs in faiss_results.items()}\n",
    "        return response['text'], retrieved_doc_ids\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing query: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Flight 624 faced an accident when the aircraft struck power lines and an antenna array upon approach in heavy snow at Halifax Stanfield International Airport. The hard landing resulted in severe damage to the aircraft, but all passengers and crew evacuated safely, with 25 people sustaining minor injuries.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the generative model\n",
    "generative_model = Ollama(model=\"llama3:8b\")\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Given the following context, provide a concise and relevant response to the user query.\n",
    "    Focus on extracting information that is related to the query and avoid irrelevant content.\n",
    "    User Query: {user_query}\n",
    "    Context: {context}\n",
    "    \"\"\")\n",
    "\n",
    "# Create the LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt_template, llm=generative_model)\n",
    "\n",
    "# Initialize connection to Neo4j\n",
    "graphdb_client = GraphDBClient(uri=os.getenv('NEO4J_URI'), user=os.getenv('NEO4J_USERNAME'), password=os.getenv('NEO4J_PASSWORD'))\n",
    "\n",
    "# Initialize the GraphDBRetriever\n",
    "graphdb_retriever = GraphDBRetriever(\n",
    "    graphdb_client=graphdb_client,\n",
    "    faiss_retrievers={\n",
    "        \"United Airlines\": united_faiss_retriever,\n",
    "        \"Alliance Airlines\": alliance_faiss_retriever,\n",
    "        \"Air Canada\": air_canada_faiss_retriever\n",
    "    }\n",
    ")\n",
    "\n",
    "user_query = \"Tell me about what aircraft FLIGHT 624 faced accident\"\n",
    "response,sample_doc_ret = process_query(user_query, graphdb_retriever, llm_chain)\n",
    "print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: There is no specific record of an accident at Halifax Stanfield International Airport. However, Air Canada Flight 624 did experience a hard landing upon approach to the airport in heavy snow conditions, resulting in severe damage to the aircraft. All passengers and crew evacuated safely, with 25 people sustaining minor injuries.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Tell me what happened on Halifax Stanfield International Airport\"\n",
    "response,sample_doc_ret = process_query(user_query, graphdb_retriever, llm_chain)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'United Airlines': ['united_7', 'united_4', 'united_6', 'united_5', 'united_1', 'united_3', 'united_0', 'united_2'], 'Alliance Airlines': ['alliance_4', 'alliance_5', 'alliance_6', 'alliance_7', 'alliance_0', 'alliance_3', 'alliance_1', 'alliance_2'], 'Air Canada': ['air_canada_5', 'air_canada_6', 'air_canada_4', 'air_canada_7', 'air_canada_2', 'air_canada_3', 'air_canada_1', 'air_canada_0']}\n"
     ]
    }
   ],
   "source": [
    "print(sample_doc_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_query = \"Tell me about what aircraft FLIGHT 328\"\n",
    "#response = process_query(user_query, custom_multi_retriever, graphdb_client, llm_chain)\n",
    "#print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "\n",
    "def precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_set = set(relevant_docs)\n",
    "    retrieved_k = retrieved_docs[:k]\n",
    "    retrieved_set = set(retrieved_k)\n",
    "    intersection = retrieved_set.intersection(relevant_set)\n",
    "    precision = len(intersection) / len(retrieved_k) if retrieved_k else 0\n",
    "    return precision\n",
    "\n",
    "def mean_reciprocal_rank(retrieved_docs, relevant_docs):\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        if doc in relevant_docs:\n",
    "            return 1 / (i + 1)\n",
    "    return 0\n",
    "\n",
    "def ndcg_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    dcg = 0\n",
    "    for i, doc in enumerate(retrieved_docs[:k]):\n",
    "        if doc in relevant_docs:\n",
    "            dcg += 1 / np.log2(i + 2)\n",
    "    idcg = sum(1 / np.log2(i + 2) for i in range(min(len(relevant_docs), k)))\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "def bleu_score(reference, hypothesis):\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    reference_tokens = reference.split()\n",
    "    hypothesis_tokens = hypothesis.split()\n",
    "    return sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function)\n",
    "\n",
    "# Function to evaluate multiple metrics\n",
    "def evaluate_performance(retrieved_docs, relevant_docs, generated_response, reference_response, k=10):\n",
    "    precision = precision_at_k(retrieved_docs, relevant_docs, k)\n",
    "    mrr = mean_reciprocal_rank(retrieved_docs, relevant_docs)\n",
    "    ndcg = ndcg_at_k(retrieved_docs, relevant_docs, k)\n",
    "    bleu = bleu_score(reference_response, generated_response)\n",
    "    return precision, mrr, ndcg, bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'United Airlines': ['united_4', 'united_7', 'united_5', 'united_6', 'united_3', 'united_1', 'united_0', 'united_2'], 'Alliance Airlines': ['alliance_7', 'alliance_6', 'alliance_5', 'alliance_4', 'alliance_3', 'alliance_0', 'alliance_2', 'alliance_1'], 'Air Canada': ['air_canada_6', 'air_canada_4', 'air_canada_5', 'air_canada_7', 'air_canada_2', 'air_canada_3', 'air_canada_0', 'air_canada_1']}\n",
      "{'United Airlines': ['united_5', 'united_4', 'united_7', 'united_3', 'united_6', 'united_1', 'united_0', 'united_2'], 'Alliance Airlines': ['alliance_6', 'alliance_7', 'alliance_4', 'alliance_5', 'alliance_3', 'alliance_0', 'alliance_2', 'alliance_1'], 'Air Canada': ['air_canada_6', 'air_canada_4', 'air_canada_7', 'air_canada_5', 'air_canada_3', 'air_canada_2', 'air_canada_0', 'air_canada_1']}\n",
      "{'United Airlines': ['united_6', 'united_4', 'united_5', 'united_7', 'united_3', 'united_1', 'united_2', 'united_0'], 'Alliance Airlines': ['alliance_5', 'alliance_6', 'alliance_7', 'alliance_4', 'alliance_3', 'alliance_0', 'alliance_2', 'alliance_1'], 'Air Canada': ['air_canada_6', 'air_canada_5', 'air_canada_4', 'air_canada_7', 'air_canada_2', 'air_canada_3', 'air_canada_0', 'air_canada_1']}\n",
      "{'United Airlines': ['united_5', 'united_7', 'united_6', 'united_4', 'united_3', 'united_1', 'united_2', 'united_0'], 'Alliance Airlines': ['alliance_6', 'alliance_5', 'alliance_4', 'alliance_7', 'alliance_3', 'alliance_0', 'alliance_1', 'alliance_2'], 'Air Canada': ['air_canada_4', 'air_canada_6', 'air_canada_5', 'air_canada_7', 'air_canada_3', 'air_canada_2', 'air_canada_0', 'air_canada_1']}\n",
      "{'United Airlines': ['united_5', 'united_4', 'united_6', 'united_7', 'united_3', 'united_1', 'united_2', 'united_0'], 'Alliance Airlines': ['alliance_4', 'alliance_5', 'alliance_6', 'alliance_7', 'alliance_3', 'alliance_0', 'alliance_2', 'alliance_1'], 'Air Canada': ['air_canada_5', 'air_canada_4', 'air_canada_6', 'air_canada_7', 'air_canada_3', 'air_canada_2', 'air_canada_0', 'air_canada_1']}\n",
      "{'United Airlines': ['united_5', 'united_4', 'united_7', 'united_6', 'united_1', 'united_3', 'united_2', 'united_0'], 'Alliance Airlines': ['alliance_6', 'alliance_4', 'alliance_5', 'alliance_7', 'alliance_3', 'alliance_0', 'alliance_2', 'alliance_1'], 'Air Canada': ['air_canada_6', 'air_canada_4', 'air_canada_7', 'air_canada_5', 'air_canada_3', 'air_canada_2', 'air_canada_0', 'air_canada_1']}\n",
      "{'United Airlines': ['united_5', 'united_6', 'united_7', 'united_4', 'united_3', 'united_1', 'united_2', 'united_0'], 'Alliance Airlines': ['alliance_4', 'alliance_6', 'alliance_5', 'alliance_7', 'alliance_3', 'alliance_2', 'alliance_0', 'alliance_1'], 'Air Canada': ['air_canada_6', 'air_canada_5', 'air_canada_7', 'air_canada_4', 'air_canada_2', 'air_canada_3', 'air_canada_0', 'air_canada_1']}\n",
      "{'United Airlines': ['united_6', 'united_5', 'united_7', 'united_4', 'united_3', 'united_1', 'united_2', 'united_0'], 'Alliance Airlines': ['alliance_5', 'alliance_3', 'alliance_6', 'alliance_7', 'alliance_4', 'alliance_0', 'alliance_2', 'alliance_1'], 'Air Canada': ['air_canada_6', 'air_canada_5', 'air_canada_4', 'air_canada_7', 'air_canada_3', 'air_canada_2', 'air_canada_0', 'air_canada_1']}\n",
      "{'United Airlines': ['united_5', 'united_6', 'united_4', 'united_7', 'united_3', 'united_1', 'united_2', 'united_0'], 'Alliance Airlines': ['alliance_6', 'alliance_5', 'alliance_4', 'alliance_7', 'alliance_3', 'alliance_0', 'alliance_2', 'alliance_1'], 'Air Canada': ['air_canada_6', 'air_canada_4', 'air_canada_7', 'air_canada_5', 'air_canada_2', 'air_canada_3', 'air_canada_0', 'air_canada_1']}\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"Tell me about United Airlines Flight 232\",\n",
    "        \"relevant_docs\": [\"united_0\",\"united_1\",\"united_2\"],\n",
    "        \"expected_response\": \"United Airlines Flight 232 suffered an uncontained engine failure, leading to the loss of all hydraulic systems. The aircraft broke apart upon landing. Out of 296 passengers and crew, 111 were killed, and 185 survived.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Details of United Airlines Flight 93\",\n",
    "        \"relevant_docs\": [\"united_3\",\"united_4\",\"united_5\"],\n",
    "        \"expected_response\": \"United Airlines Flight 93 was hijacked during the September 11 attacks. The plane crashed into a field, killing all 44 people on board. The passengers' actions prevented the aircraft from reaching its intended target.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Incident with United Airlines Flight 328\",\n",
    "        \"relevant_docs\": [\"united_6\",\"united_7\",\"united_8\"],\n",
    "        \"expected_response\": \"United Airlines Flight 328 experienced a right engine failure shortly after takeoff. The aircraft returned to Denver International Airport for an emergency landing with no injuries.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Tell me about Air Canada Flight 797\",\n",
    "        \"relevant_docs\": [\"air_canada_0\",\"air_canada_1\",\"air_canada_5\"],\n",
    "        \"expected_response\": \"Air Canada Flight 797 had a fire start in the lavatory while in flight. The aircraft landed, but the fire rapidly spread, resulting in the deaths of 23 passengers. 21 passengers and all crew members survived.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Details of Air Canada Flight 624\",\n",
    "        \"relevant_docs\": [\"air_canada_0\",\"air_canada_1\",\"air_canada_2\"],\n",
    "        \"expected_response\": \"Air Canada Flight 624 struck power lines and an antenna array upon approach in heavy snow at Halifax Stanfield International Airport. All passengers and crew evacuated safely, with 25 people sustaining minor injuries.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Incident with Air Canada Flight 759\",\n",
    "        \"relevant_docs\": [\"air_canada_3\",\"air_canada_4\",\"air_canada_5\"],\n",
    "        \"expected_response\": \"Air Canada Flight 759 was cleared to land on a runway that was closed for maintenance at San Francisco International Airport. The aircraft almost landed on a taxiway, but the incident was averted, and it landed safely on a different runway.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Tell me about Alliance Airlines Flight 764\",\n",
    "        \"relevant_docs\": [\"alliance_0\",\"alliance_1\",\"alliance_2\"],\n",
    "        \"expected_response\": \"Alliance Airlines Flight 764 suffered a hydraulic system failure shortly after takeoff. The aircraft returned to Brisbane and landed safely without any injuries.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Details of Alliance Airlines Flight 850\",\n",
    "        \"relevant_docs\": [\"alliance_3\",\"alliance_4\",\"alliance_5\"],\n",
    "        \"expected_response\": \"Alliance Airlines Flight 850 struck a flock of birds during final approach to Perth Airport, causing significant damage to the right engine. The aircraft landed safely with no injuries.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Incident with Alliance Airlines Flight 982\",\n",
    "        \"relevant_docs\": [\"alliance_6\",\"alliance_7\",\"alliance_8\"],\n",
    "        \"expected_response\": \"Alliance Airlines Flight 982 encountered severe turbulence while descending towards Melbourne. Several passengers were injured, but the aircraft landed safely.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Initialize connection to Neo4j\n",
    "graphdb_client = GraphDBClient(uri=os.getenv('NEO4J_URI'), user=os.getenv('NEO4J_USERNAME'), password=os.getenv('NEO4J_PASSWORD'))\n",
    "\n",
    "# Evaluate Performance\n",
    "for test_case in test_queries:\n",
    "    user_query = test_case[\"query\"]\n",
    "    relevant_docs = test_case[\"relevant_docs\"]\n",
    "    reference_response = test_case[\"expected_response\"]\n",
    "\n",
    "    generated_response, retrieved_docs = process_query(user_query, graphdb_retriever, llm_chain)\n",
    "    print(retrieved_docs)\n",
    "\n",
    "    # Flatten the retrieved documents\n",
    "    retrieved_doc_ids = [doc_id for doc_list in retrieved_docs.values() for doc_id in doc_list]\n",
    "\n",
    "    precision, mrr, ndcg, bleu = evaluate_performance(retrieved_doc_ids, relevant_docs, generated_response, reference_response)\n",
    "    \n",
    "    results.append({\n",
    "        \"query\": user_query,\n",
    "        \"precision@10\": precision,\n",
    "        \"mrr\": mrr,\n",
    "        \"ndcg@10\": ndcg,\n",
    "        \"bleu\": bleu,\n",
    "        \"generated_response\": generated_response\n",
    "    })\n",
    "\n",
    "graphdb_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Tell me about United Airlines Flight 232\n",
      "Precision@10: 0.3\n",
      "MRR: 0.16666666666666666\n",
      "nDCG@10: 0.4716276524567204\n",
      "BLEU: 0.19308745870320945\n",
      "Query: Details of United Airlines Flight 93\n",
      "Precision@10: 0.3\n",
      "MRR: 1.0\n",
      "nDCG@10: 0.9674679834891693\n",
      "BLEU: 0.28966464549038945\n",
      "Query: Incident with United Airlines Flight 328\n",
      "Precision@10: 0.2\n",
      "MRR: 1.0\n",
      "nDCG@10: 0.6713860725233041\n",
      "BLEU: 0.015109910211096535\n",
      "Query: Tell me about Air Canada Flight 797\n",
      "Precision@10: 0.0\n",
      "MRR: 0.05263157894736842\n",
      "nDCG@10: 0.0\n",
      "BLEU: 0.17646721154771156\n",
      "Query: Details of Air Canada Flight 624\n",
      "Precision@10: 0.0\n",
      "MRR: 0.045454545454545456\n",
      "nDCG@10: 0.0\n",
      "BLEU: 0.2363200959974121\n",
      "Query: Incident with Air Canada Flight 759\n",
      "Precision@10: 0.0\n",
      "MRR: 0.05555555555555555\n",
      "nDCG@10: 0.0\n",
      "BLEU: 0.010366337342543096\n",
      "Query: Tell me about Alliance Airlines Flight 764\n",
      "Precision@10: 0.0\n",
      "MRR: 0.07142857142857142\n",
      "nDCG@10: 0.0\n",
      "BLEU: 0.12790983849430193\n",
      "Query: Details of Alliance Airlines Flight 850\n",
      "Precision@10: 0.2\n",
      "MRR: 0.1111111111111111\n",
      "nDCG@10: 0.2769189462922768\n",
      "BLEU: 0.16198242815546376\n",
      "Query: Incident with Alliance Airlines Flight 982\n",
      "Precision@10: 0.1\n",
      "MRR: 0.1111111111111111\n",
      "nDCG@10: 0.14126697285982898\n",
      "BLEU: 0.15044546876575104\n"
     ]
    }
   ],
   "source": [
    "# Print Evaluation Results\n",
    "for result in results:\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Precision@10: {result['precision@10']}\")\n",
    "    print(f\"MRR: {result['mrr']}\")\n",
    "    print(f\"nDCG@10: {result['ndcg@10']}\")\n",
    "    print(f\"BLEU: {result['bleu']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
