{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "import faiss\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from neo4j import GraphDatabase\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.documents import Document\n",
    "from typing import List, Dict\n",
    "from pydantic import BaseModel, validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load HuggingFace pipeline for relationship extraction\n",
    "rel_extractor = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess data from various sources\n",
    "def load_data():\n",
    "    # Load CSV files\n",
    "    united_dates_locations = pd.read_csv(\"data/united_dates_locations.csv\")\n",
    "    alliance_dates_locations = pd.read_csv(\"data/alliance_dates_locations.csv\")\n",
    "    air_canada_dates_locations = pd.read_csv(\"data/air_canada_dates_locations.csv\")\n",
    "\n",
    "    # Load text files\n",
    "    with open(\"data/united_aircraft_details.txt\", \"r\") as file:\n",
    "        united_aircraft_details_content = file.read().split('\\n\\n')\n",
    "    with open(\"data/alliance_aircraft_details.txt\", \"r\") as file:\n",
    "        alliance_aircraft_details_content = file.read().split('\\n\\n')\n",
    "    with open(\"data/air_canada_aircraft_details.txt\", \"r\") as file:\n",
    "        air_canada_aircraft_details_content = file.read().split('\\n\\n')\n",
    "\n",
    "    # Extract text from PDF files\n",
    "    def extract_text_from_pdf(pdf_path):\n",
    "        text = []\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PdfReader(file)\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                text.append(reader.pages[page_num].extract_text())\n",
    "        return \"\\n\".join(text)\n",
    "\n",
    "    united_pdf_content = extract_text_from_pdf(\"data/united_accident_outcomes.pdf\")\n",
    "    alliance_pdf_content = extract_text_from_pdf(\"data/alliance_accident_outcomes.pdf\")\n",
    "    air_canada_pdf_content = extract_text_from_pdf(\"data/air_canada_accident_outcomes.pdf\")\n",
    "\n",
    "    # Combine data\n",
    "    united_data = united_dates_locations['summary'].tolist() + united_aircraft_details_content + [united_pdf_content]\n",
    "    alliance_data = alliance_dates_locations['summary'].tolist() + alliance_aircraft_details_content + [alliance_pdf_content]\n",
    "    air_canada_data = air_canada_dates_locations['summary'].tolist() + air_canada_aircraft_details_content + [air_canada_pdf_content]\n",
    "\n",
    "    return united_data, alliance_data, air_canada_data\n",
    "\n",
    "# Load the data\n",
    "united_data, alliance_data, air_canada_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model for embedding\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "def create_faiss_retriever(index_path, model_name, data_list):\n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        index = faiss.read_index(index_path)\n",
    "        \n",
    "        # Initialize the document store and populate with documents\n",
    "        docs = {}\n",
    "        index_to_docstore_id = {}\n",
    "        \n",
    "        for i, text in enumerate(data_list):\n",
    "            doc_id = str(i)\n",
    "            docs[doc_id] = Document(page_content=text)\n",
    "            index_to_docstore_id[i] = doc_id\n",
    "        \n",
    "        docstore = InMemoryDocstore(docs)\n",
    "        \n",
    "        return FAISS(embedding_function=embeddings, index=index, docstore=docstore, index_to_docstore_id=index_to_docstore_id)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating FAISS retriever: {e}\")\n",
    "        raise\n",
    "\n",
    "# Create FAISS indexes\n",
    "def vectorize_and_index(data_list, index_path):\n",
    "    try:\n",
    "        embeddings = np.vstack([embed_text(text) for text in data_list])\n",
    "        create_faiss_index(embeddings, index_path)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in vectorize_and_index: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_faiss_index(data, index_path):\n",
    "    try:\n",
    "        d = data.shape[1]\n",
    "        index = faiss.IndexFlatL2(d)\n",
    "        index.add(data.astype('float32'))\n",
    "        faiss.write_index(index, index_path)\n",
    "        logger.info(f\"FAISS index created at {index_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating FAISS index: {e}\")\n",
    "        raise\n",
    "\n",
    "# Generate embeddings and create FAISS indexes\n",
    "vectorize_and_index(united_data, \"faiss_indexes/united_faiss.index\")\n",
    "vectorize_and_index(alliance_data, \"faiss_indexes/alliance_faiss.index\")\n",
    "vectorize_and_index(air_canada_data, \"faiss_indexes/air_canada_faiss.index\")\n",
    "\n",
    "# Load FAISS indexes and create retrievers\n",
    "united_faiss = create_faiss_retriever(\"faiss_indexes/united_faiss.index\", \"sentence-transformers/all-MiniLM-L6-v2\", united_data)\n",
    "alliance_faiss = create_faiss_retriever(\"faiss_indexes/alliance_faiss.index\", \"sentence-transformers/all-MiniLM-L6-v2\", alliance_data)\n",
    "air_canada_faiss = create_faiss_retriever(\"faiss_indexes/air_canada_faiss.index\", \"sentence-transformers/all-MiniLM-L6-v2\", air_canada_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_and_relationships(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    relationships = []\n",
    "    for sent in doc.sents:\n",
    "        if len(sent.ents) > 1:\n",
    "            pairs = [(ent1.text, ent2.text) for ent1 in sent.ents for ent2 in sent.ents if ent1 != ent2]\n",
    "            for pair in pairs:\n",
    "                rel = rel_extractor(\" \".join([pair[0], pair[1]]), candidate_labels=[\"caused by\", \"led to\", \"related to\"])\n",
    "                relationships.append((pair[0], rel['labels'][0], pair[1]))\n",
    "    return entities, relationships\n",
    "\n",
    "class GraphDBClient:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.uri = uri\n",
    "        self.username = user\n",
    "        self.password = password\n",
    "        self.driver = None\n",
    "        try:\n",
    "            self.connect()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to connect to Neo4j: {e}\")\n",
    "            raise ConnectionError(f\"Failed to connect to Neo4j: {e}\")\n",
    "\n",
    "    def connect(self):\n",
    "        self.driver = GraphDatabase.driver(self.uri, auth=(self.username, self.password))\n",
    "        logger.info(\"Successfully connected to Neo4j.\")\n",
    "\n",
    "    def query(self, query, parameters=None):\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(query, parameters)\n",
    "                return [record for record in result]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error executing query in Neo4j: {e}\")\n",
    "            raise\n",
    "\n",
    "# Usage\n",
    "try:\n",
    "    graph_client = GraphDBClient(os.getenv('NEO4J_URI'), os.getenv('NEO4J_USERNAME'), os.getenv('NEO4J_PASSWORD'))\n",
    "    results = graph_client.query(\"MATCH (n) RETURN n LIMIT 5\")\n",
    "    logger.info(f\"Query results: {results}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred during Neo4j operations: {e}\")\n",
    "\n",
    "# Connect to the Neo4j database\n",
    "graphdb_client = GraphDBClient(uri=os.getenv('NEO4J_URI'), user=os.getenv('NEO4J_USERNAME'), password=os.getenv('NEO4J_PASSWORD'))\n",
    "\n",
    "def create_knowledge_graph(conn, datasets):\n",
    "    for airline, data in datasets.items():\n",
    "        for entry in data:\n",
    "            entity_name = entry  # Using the text entry as the entity name\n",
    "            \n",
    "            # Extract entities and relationships\n",
    "            entities, relationships = extract_entities_and_relationships(entity_name)\n",
    "            \n",
    "            # Create Airline node\n",
    "            conn.query(\"\"\"\n",
    "            MERGE (a:Airline {name: $airline})\n",
    "            RETURN a\n",
    "            \"\"\", parameters={'airline': airline})\n",
    "            logger.info(f\"Airline node created for {airline}\")\n",
    "            \n",
    "            # Create Accident node\n",
    "            conn.query(\"\"\"\n",
    "            MERGE (acc:Accident {name: $entity_name, content: $content})\n",
    "            RETURN acc\n",
    "            \"\"\", parameters={'entity_name': entity_name, 'content': entity_name})\n",
    "            logger.info(f\"Accident node created for {entity_name}\")\n",
    "            \n",
    "            # Create relationships between Airline and Accident\n",
    "            conn.query(\"\"\"\n",
    "            MATCH (a:Airline {name: $airline})\n",
    "            MATCH (acc:Accident {name: $entity_name})\n",
    "            MERGE (a)-[:HAS_ACCIDENT]->(acc)\n",
    "            \"\"\", parameters={'airline': airline, 'entity_name': entity_name})\n",
    "            logger.info(f\"Relationship created between Airline {airline} and Accident {entity_name}\")\n",
    "            \n",
    "            # Create entities and relationships to the Accident\n",
    "            for entity_text, entity_label in entities:\n",
    "                conn.query(\"\"\"\n",
    "                MERGE (e:Entity {name: $entity, type: $type})\n",
    "                MERGE (acc:Accident {name: $entity_name})-[:MENTIONS]->(e)\n",
    "                \"\"\", parameters={'entity': entity_text, 'type': entity_label, 'entity_name': entity_name})\n",
    "                logger.info(f\"Entity node created for {entity_text} with label {entity_label}, related to Accident {entity_name}\")\n",
    "            \n",
    "            # Create relationships between entities based on extracted relationships\n",
    "            for entity1, relation, entity2 in relationships:\n",
    "                conn.query(\"\"\"\n",
    "                MATCH (e1:Entity {name: $entity1}), (e2:Entity {name: $entity2})\n",
    "                MERGE (e1)-[r:RELATION {type: $relation, source: $source}]->(e2)\n",
    "                \"\"\", parameters={'entity1': entity1, 'entity2': entity2, 'relation': relation, 'source': airline})\n",
    "                logger.info(f\"Relationship {relation} created between Entity {entity1} and Entity {entity2} for Airline {airline}\")\n",
    "\n",
    "\n",
    "# Combine all datasets\n",
    "datasets = {\n",
    "    \"United_Airlines\": united_data,\n",
    "    \"Alliance_Airlines\": alliance_data,\n",
    "    \"Air_Canada\": air_canada_data\n",
    "}\n",
    "create_knowledge_graph(graphdb_client, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_knowledge_graph(client, keyword):\n",
    "    query = \"\"\"\n",
    "    MATCH (e:Entity)-[r]->(related:Entity)\n",
    "    WHERE e.name =~ $name OR related.name =~ $name\n",
    "    RETURN e.name AS entity, type(r) AS relationship, related.name AS related_entity\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = client.session().run(query, name=keyword)\n",
    "        return [record for record in result]\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying knowledge graph: {e}\")\n",
    "        return []\n",
    "    \n",
    "class VectorStoreRetriever:\n",
    "    def __init__(self, index):\n",
    "        self.index = index  # Assume this is a FAISS index\n",
    "\n",
    "    def search(self, query_vector, k=10):\n",
    "        # Ensure query_vector is a numpy array and is in the correct shape\n",
    "        distances, indices = self.index.search(np.array([query_vector]), k)  # Retrieve the top-k closest vectors\n",
    "        return distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(query):\n",
    "    doc = nlp(query)\n",
    "    keywords = [token.text for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return keywords\n",
    "\n",
    "class CustomMultiRetriever(BaseModel):\n",
    "    faiss_retrievers: Dict[str, Any]\n",
    "    knowledge_graph_client: Any\n",
    "\n",
    "    # If you need to perform any checks or initializations post-creation, use validators or root validators\n",
    "    @validator('faiss_retrievers')\n",
    "    def check_faiss_retrievers(cls, value):\n",
    "        if not value:\n",
    "            raise ValueError(\"FAISS retrievers cannot be empty\")\n",
    "        return value\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        # Example method that uses the initialized FAISS retrievers and the knowledge graph client\n",
    "        results = {}\n",
    "        for airline, retriever in self.faiss_retrievers.items():\n",
    "            # Simulate a search operation; ensure the retriever has a method that can handle the search\n",
    "            distances, indices = retriever.search(query)  # Your retriever must have a search method\n",
    "            results[airline] = (distances, indices)\n",
    "        return results\n",
    "\n",
    "custom_multi_retriever = CustomMultiRetriever(\n",
    "    faiss_retrievers={\n",
    "        \"United Airlines\": united_faiss.as_retriever(),\n",
    "        \"Alliance Airlines\": alliance_faiss.as_retriever,\n",
    "        \"Air Canada\": air_canada_faiss.as_retriever\n",
    "    },\n",
    "    knowledge_graph_client=graphdb_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an AI assistant that specializes in providing detailed information about airline accidents. \n",
    "    When given a query about a specific flight, you should:\n",
    "    \n",
    "    1. Identify the flight number and any other relevant details from the query.\n",
    "    2. Retrieve specific information about the flight from the provided context, including any relevant accidents or incidents.\n",
    "    3. Summarize the information in a clear and concise manner.\n",
    "    4. If there are multiple incidents related to the flight, provide details on each incident separately.\n",
    "    5. Ensure the response is focused on the specific flight mentioned in the query.\n",
    "\n",
    "    Use the provided context to generate the response and avoid including unrelated information.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\"]\n",
    ")\n",
    "\n",
    "# Initialize the generative model\n",
    "generative_model = Ollama(model=\"gemma:7b\")\n",
    "\n",
    "# Create the LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt_template, llm=generative_model)\n",
    "\n",
    "def integrate_graph_and_docs(graph_results, retrieved_docs):\n",
    "    integrated_context = \"Retrieved Context:\\n\"\n",
    "    if graph_results:\n",
    "        for result in graph_results:\n",
    "            entity = result.get('entity', 'Unknown Entity')\n",
    "            relationship = result.get('relationship', 'Unknown Relationship')\n",
    "            related_entity = result.get('related_entity', 'Unknown Related Entity')\n",
    "            integrated_context += f\"{entity} {relationship} {related_entity}. \"\n",
    "    else:\n",
    "        integrated_context += \"No relevant data found in the knowledge graph.\\n\"\n",
    "        \n",
    "    if retrieved_docs:\n",
    "        integrated_context += \"\\nFrom FAISS:\\n\"\n",
    "        for doc in retrieved_docs:\n",
    "            integrated_context += doc.page_content + \"\\n\"\n",
    "    else:\n",
    "        integrated_context += \"No relevant data found in the FAISS retrievers.\\n\"\n",
    "    \n",
    "    return integrated_context\n",
    "\n",
    "def process_query(user_query, custom_multi_retriever, graphdb_client, llm_chain):\n",
    "    try:\n",
    "        entities = extract_entities(user_query)\n",
    "        logger.info(f\"Extracted entities: {entities}\")\n",
    "\n",
    "        graph_context = \"\"\n",
    "        for entity in entities:\n",
    "            graph_results = query_knowledge_graph(graphdb_client, entity)\n",
    "            logger.info(f\"Graph results for entity {entity}: {graph_results}\")\n",
    "            if (len(graph_results) > 0):\n",
    "                graph_context += integrate_graph_and_docs(graph_results, [])\n",
    "            else:\n",
    "                graph_context += f\"No relevant data found in the knowledge graph for entity: {entity}.\\n\"\n",
    "\n",
    "        faiss_docs = custom_multi_retriever.get_relevant_documents(user_query)\n",
    "        logger.info(f\"FAISS results: {faiss_docs}\")\n",
    "\n",
    "        final_context = graph_context + \" \".join([doc.page_content for docs in faiss_docs.values() for doc in docs])\n",
    "        logger.info(f\"Final integrated context: {final_context}\")\n",
    "\n",
    "        response = llm_chain({\"context\": final_context})\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing query: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "user_query = \"Tell me about what aircraft faced accident on this day\"\n",
    "response = process_query(user_query)\n",
    "logger.info(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
